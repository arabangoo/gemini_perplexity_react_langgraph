"""Perplexity + Gemini Research Agent (MemorySaver)"""
import os
from typing import Literal
from langgraph.graph import StateGraph, START, END
# MemorySaver ì œê±° - LangGraph APIê°€ persistence ìë™ ì²˜ë¦¬
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage
from agent.state import ResearchState
from tools.perplexity import perplexity_search

# Gemini ì´ˆê¸°í™”
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY not found in environment")

gemini = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.3,
    api_key=GEMINI_API_KEY
)

def extract_query(state: ResearchState) -> ResearchState:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì¿¼ë¦¬ ì¶”ì¶œ"""
    messages = state.get("messages", [])
    if messages:
        last_message = messages[-1]
        if hasattr(last_message, 'content'):
            state["query"] = last_message.content
    state.setdefault("iteration", 0)
    state.setdefault("search_results", [])
    state.setdefault("citations", [])
    state.setdefault("related_questions", [])
    return state


async def search_perplexity(state: ResearchState) -> ResearchState:
    """Perplexityë¡œ ì›¹ ê²€ìƒ‰"""
    query = state["query"]
    print(f"\nğŸ” [ê²€ìƒ‰ {state['iteration'] + 1}] Perplexity: {query}")
    result = await perplexity_search.ainvoke({"query": query, "search_recency": "month"})
    if "error" in result:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {result['error']}")
        result = {"content": "", "citations": [], "related_questions": []}
    state["search_results"].append(result)
    state["citations"].extend(result.get("citations", []))
    state["related_questions"] = result.get("related_questions", [])
    state["iteration"] += 1
    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(result.get('citations', []))}ê°œ ì¶œì²˜")
    return state

async def analyze_with_gemini(state: ResearchState) -> ResearchState:
    """Geminië¡œ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
    query = state["query"]
    all_content = "\n\n".join([r.get('content', '') for r in state["search_results"] if r.get('content')])
    if not all_content:
        state["analysis"] = "No results"
        state["needs_more_research"] = False
        return state
    print(f"\nğŸ§  Gemini ë¶„ì„ ì¤‘...")
    prompt = f"ì§ˆë¬¸: {query}\n\nê²€ìƒ‰ê²°ê³¼:\n{all_content}\n\nì •ë³´ê°€ ì¶©ë¶„í•˜ë©´ SUFFICIENT: YES, ë¶€ì¡±í•˜ë©´ SUFFICIENT: NO"
    try:
        response = await gemini.ainvoke([HumanMessage(content=prompt)])
        state["analysis"] = response.content
        state["needs_more_research"] = "SUFFICIENT: NO" in response.content.upper() and state["iteration"] < 3
        print(f"âœ… ë¶„ì„ ì™„ë£Œ | ì¶”ê°€ ê²€ìƒ‰: {state['needs_more_research']}")
    except Exception as e:
        print(f"âŒ Gemini ì˜¤ë¥˜: {str(e)}")
        state["analysis"] = f"Error: {str(e)}"
        state["needs_more_research"] = False
    return state


async def generate_final_answer(state: ResearchState) -> ResearchState:
    """ìµœì¢… ë‹µë³€ ìƒì„±"""
    query = state["query"]
    all_content = "\n\n".join([r.get('content', '') for r in state["search_results"] if r.get('content')])
    print(f"\nğŸ“ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
    if not all_content:
        answer = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    else:
        prompt = f"ì§ˆë¬¸: {query}\n\nê²€ìƒ‰ ì •ë³´:\n{all_content}\n\nëª…í™•í•˜ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:"
        try:
            response = await gemini.ainvoke([HumanMessage(content=prompt)])
            answer = response.content
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    citations = list(set(state["citations"]))
    if citations:
        answer += "\n\n---\n**ğŸ“š ì°¸ê³  ì¶œì²˜:**\n"
        for i, cite in enumerate(citations[:10], 1):
            answer += f"\n[{i}] {cite}"
    if state.get("related_questions"):
        answer += "\n\n---\n**ğŸ”— ê´€ë ¨ ì§ˆë¬¸:**\n"
        for q in state["related_questions"][:5]:
            answer += f"\nâ€¢ {q}"
    state["final_answer"] = answer
    state["messages"].append(AIMessage(content=answer))
    print(f"âœ… ìµœì¢… ë‹µë³€ ì™„ë£Œ")
    return state

def should_continue(state: ResearchState) -> Literal["search", "answer"]:
    """ì¶”ê°€ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€"""
    return "search" if state.get("needs_more_research", False) else "answer"

def create_research_graph():
    """ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ìƒì„± (LangGraph API í˜¸í™˜)"""
    workflow = StateGraph(ResearchState)
    workflow.add_node("extract", extract_query)
    workflow.add_node("search", search_perplexity)
    workflow.add_node("analyze", analyze_with_gemini)
    workflow.add_node("answer", generate_final_answer)
    workflow.add_edge(START, "extract")
    workflow.add_edge("extract", "search")
    workflow.add_edge("search", "analyze")
    workflow.add_conditional_edges("analyze", should_continue, {"search": "search", "answer": "answer"})
    workflow.add_edge("answer", END)
    print("ğŸ’¾ ëŒ€í™” ì €ì¥: LangGraph API ìë™ ê´€ë¦¬")
    return workflow.compile()  # checkpointer ì œê±° - LangGraph APIê°€ ìë™ ì²˜ë¦¬

research_graph = create_research_graph()
graph = research_graph  # langgraph.json í˜¸í™˜ì„±ì„ ìœ„í•œ alias
